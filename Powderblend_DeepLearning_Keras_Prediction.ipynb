{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Powderblend_DeepLearning_Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swayamprakashpatel/PU_DL/blob/main/Powderblend_DeepLearning_Keras_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPu9WZxHUc4A"
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from  matplotlib import pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
        "from tensorflow.keras.applications.imagenet_utils import decode_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBI7lNIlrSSx"
      },
      "source": [
        "%%capture\n",
        "#Training Folder Download\n",
        "def folder_download(folder_id):\n",
        "  # authenticate\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "  # get folder_name\n",
        "  from googleapiclient.discovery import build\n",
        "  service = build('drive', 'v3')\n",
        "  folder_name = service.files().get(fileId=folder_id).execute()['name']\n",
        "  # import library and download\n",
        "  !wget -qnc https://github.com/segnolin/google-drive-folder-downloader/raw/master/download.py\n",
        "  from download import download_folder\n",
        "  download_folder(service, folder_id, '/content/sample_data/', folder_name)\n",
        "  return folder_name\n",
        "folder_download('1zZFaYBPEAt-Os0hV1eqXSd8v9SoC-eja')\n",
        "\n",
        "#Validation Folder Download\n",
        "def folder_download(folder_id):\n",
        "  # authenticate\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "  # get folder_name\n",
        "  from googleapiclient.discovery import build\n",
        "  service = build('drive', 'v3')\n",
        "  folder_name = service.files().get(fileId=folder_id).execute()['name']\n",
        "  # import library and download\n",
        "  !wget -qnc https://github.com/segnolin/google-drive-folder-downloader/raw/master/download.py\n",
        "  from download import download_folder\n",
        "  download_folder(service, folder_id, '/content/sample_data/', folder_name)\n",
        "  return folder_name\n",
        "folder_download('1vRM-HJ7cXzRMBVpzFKZq6QSIqKh5b_yh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQS42oxrUlKQ"
      },
      "source": [
        "#Set Batch Size and Image Height and Image Width\n",
        "batch_size = 10\n",
        "IMG_HEIGHT, IMG_WIDTH = (100,100)\n",
        "\n",
        "\n",
        "# Show image after Size Setting (Reduction)\n",
        "\n",
        "# Images of Not Mixed Powder Samples\n",
        "plt.figure(figsize=(IMG_HEIGHT,IMG_WIDTH))\n",
        "img_folder=r'/content/sample_data/Training/NOT MIXED'\n",
        "for i in range(3):\n",
        "    file = random.choice(os.listdir(img_folder))\n",
        "    image_path= os.path.join(img_folder, file)\n",
        "    img=mpimg.imread(image_path,cv2.COLOR_RGB2BGR)\n",
        "    ax=plt.subplot(1,5,i+1)\n",
        "    ax.title.set_text(file)\n",
        "    plt.imshow(img)\n",
        "\n",
        "# Images of Mixed Powder Samples\n",
        "plt.figure(figsize=(IMG_HEIGHT,IMG_WIDTH))\n",
        "img_folder=r'/content/sample_data/Training/MIXED'\n",
        "for i in range(3):\n",
        "    file = random.choice(os.listdir(img_folder))\n",
        "    image_path= os.path.join(img_folder, file)\n",
        "    img=mpimg.imread(image_path,cv2.COLOR_RGB2BGR)\n",
        "    ax=plt.subplot(1,5,i+1)\n",
        "    ax.title.set_text(file)\n",
        "    plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFgPbskR2gRD"
      },
      "source": [
        "#JUST FOR UNDERSTANDING : HOW IMAGE GET COVERTED TO DATA\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import array_to_img\n",
        "# load the image\n",
        "img = load_img('/content/WhatsApp_Image_2021-08-18_at_3.34.57_PM__10__2.jpeg', grayscale= False)\n",
        "print(type(img))\n",
        "# convert to numpy array\n",
        "img_array = img_to_array(img)\n",
        "print(img_array.dtype)\n",
        "print(img_array.shape)\n",
        "# convert back to image\n",
        "img_pil = array_to_img(img_array)\n",
        "print(type(img))\n",
        "plt.imshow(img_pil)\n",
        "print(img_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iZ6TU1xUfDB"
      },
      "source": [
        "#Directories with Subdirectories as Classes for training and validation datasets\n",
        "\n",
        "train_dir = '/content/sample_data/Training'\n",
        "validation_dir = '/content/sample_data/Validation'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_P-QdvsUfEh"
      },
      "source": [
        "#Image to Data Transform using ImageDataGenerator of Keras\n",
        "\n",
        "#Image to Data for Training Data\n",
        "Dataset_Image_Training = ImageDataGenerator(rescale = 1./255, zoom_range=[0.8, 1.5], brightness_range= [0.8, 2.0])\n",
        "train_data_gen =  Dataset_Image_Training.flow_from_directory(\n",
        "                    batch_size= batch_size,\n",
        "                    directory=train_dir,\n",
        "                    shuffle=True,\n",
        "                    target_size=(IMG_HEIGHT,IMG_WIDTH),\n",
        "                    class_mode='binary',\n",
        "                    )\n",
        "#Image to Data for Validation Data\n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255, zoom_range=[0.8, 1.5], brightness_range= [0.8, 2.0])\n",
        "val_data_gen = validation_image_generator.flow_from_directory(\n",
        "                 batch_size=batch_size,\n",
        "                 directory= validation_dir,\n",
        "                 shuffle=True,\n",
        "                 target_size=(IMG_HEIGHT,IMG_WIDTH),\n",
        "                 class_mode= 'binary')\n",
        "#Check Classes in Dataset\n",
        "train_data_gen.class_indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6f_1troU2Q0"
      },
      "source": [
        "#Deep Learning CNN Model with Keras Seqential\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
        "    MaxPool2D(2,2),\n",
        "    Conv2D(64, (3,3), padding='same', activation='relu'),\n",
        "    MaxPool2D(2,2),\n",
        "    Conv2D(128, (3,3), padding='same', activation='relu'),\n",
        "    MaxPool2D(2,2),\n",
        "    Conv2D(256, (3,3), padding='same', activation='relu'),\n",
        "    MaxPool2D(2,2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')])\n",
        "\n",
        "# Model Compilation\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Tensorboard Set up\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "#Checkpoint and earlystop setting\n",
        "filepath = '/content/drive/My Drive/DL_Model.hdf5'\n",
        "checkpoint = [tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', mode='max', save_best_only=True, Save_weights_only = False, verbose = 1), \n",
        "              tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience = 15, verbose =1), [tensorboard_callback]]\n",
        "\n",
        "#Model Fitting\n",
        "hist = model.fit(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=None,\n",
        "    epochs=500,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=None,\n",
        "    callbacks = [checkpoint],\n",
        ")\n",
        "\n",
        "#Accuracy Print\n",
        "\n",
        "train_acc = max(hist.history['accuracy'])\n",
        "val_acc = max(hist.history['val_accuracy'])\n",
        "train_loss = min(hist.history['loss'])\n",
        "val_loss = min(hist.history['val_loss'])\n",
        "print('Training accuracy is')\n",
        "print(train_acc)\n",
        "print('Validation accuracy is')\n",
        "print(val_acc)\n",
        "print('Training loss is')\n",
        "print(train_loss)\n",
        "print('Validation loss is')\n",
        "print(val_loss)\n",
        "\n",
        "#Load Tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RIkKSIOU8sI"
      },
      "source": [
        "#Prediction\n",
        "batch_size = 10\n",
        "IMG_HEIGHT, IMG_WIDTH = (100,100)\n",
        "Pred_Dir = '/content/drive/MyDrive/Model_DE/Test'\n",
        "Prediction = ImageDataGenerator(rescale = 1./255)\n",
        "Pred_Data =  Prediction.flow_from_directory(\n",
        "                    directory=Pred_Dir,\n",
        "                    batch_size= batch_size,\n",
        "                    shuffle=False,\n",
        "                    target_size=(IMG_HEIGHT,IMG_WIDTH),\n",
        "                    class_mode=None)\n",
        "#Pred_Data.reset()\n",
        "from keras.models import load_model\n",
        "model = load_model('/content/drive/MyDrive/DL_Model.hdf5')\n",
        "Prediction = np.round(model.predict(Pred_Data))\n",
        "Prediction = np.any(Prediction, axis=1)\n",
        "\n",
        "train_data_gen.class_indices\n",
        "\n",
        "#Multiple images' prediction with image caption\n",
        "\n",
        "labels = ({'MIXED': 0, 'NOT MIXED': 1})\n",
        "labels = dict((v,k) for k,v in labels.items())\n",
        "predictions = [labels[k] for k in Prediction]\n",
        "filenames = Pred_Data.filenames\n",
        "results = pd.DataFrame({\"Filename\":filenames,\n",
        "                      \"Predictions\":predictions})\n",
        "print(results)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}