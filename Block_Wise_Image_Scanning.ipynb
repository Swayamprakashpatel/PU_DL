{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6bShBeCAGGVCamGmJa8X5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swayamprakashpatel/PU_DL/blob/main/Block_Wise_Image_Scanning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TR5hgGBhD_26"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn.utils import shuffle\n",
        "%matplotlib inline\n",
        "\n",
        "# Function to extract features from an image using a sliding window\n",
        "def extract_features(image, window_size, stride):\n",
        "    features = []\n",
        "    image_height, image_width, _ = image.shape\n",
        "\n",
        "    for y in range(0, image_height - window_size[0] + 1, stride):\n",
        "        for x in range(0, image_width - window_size[1] + 1, stride):\n",
        "            window = image[y:y+window_size[0], x:x+window_size[1]]\n",
        "            # Process the window to extract features (you can modify this)\n",
        "            # For simplicity, we'll flatten the window as a feature\n",
        "            features.append(window.flatten())\n",
        "\n",
        "    return np.array(features)\n",
        "\n",
        "# Set Batch Size and Image Height and Image Width\n",
        "batch_size = 10\n",
        "IMG_HEIGHT, IMG_WIDTH = (100, 100)\n",
        "WINDOW_SIZE = (50, 50)  # Adjust this as needed\n",
        "STRIDE = 25  # Adjust this as needed\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(64, (3,3), padding='same', activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(128, (3,3), padding='same', activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(256, (3,3), padding='same', activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Model Compilation\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Directory paths for your dataset\n",
        "train_dir = 'path_to_your_training_data_directory'\n",
        "validation_dir = 'path_to_your_validation_data_directory'\n",
        "\n",
        "# ImageDataGenerator for data augmentation\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Generate batches of augmented data from the directories\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "hist = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.n // batch_size,\n",
        "    epochs=5,  # Adjust the number of epochs as needed\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.n // batch_size\n",
        ")\n",
        "\n",
        "# Accuracy Print\n",
        "train_acc = max(hist.history['accuracy'])\n",
        "val_acc = max(hist.history['val_accuracy'])\n",
        "train_loss = min(hist.history['loss'])\n",
        "val_loss = min(hist.history['val_loss'])\n",
        "print('Training accuracy is', train_acc)\n",
        "print('Validation accuracy is', val_acc)\n",
        "print('Training loss is', train_loss)\n",
        "print('Validation loss is', val_loss)\n"
      ]
    }
  ]
}